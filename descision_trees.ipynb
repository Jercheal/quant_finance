{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision trees and random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will build a decision tree algorithm from scratch. The idea of decision trees is to recursively find optimal splits in the data set along a hyperplane in the feature space. The optimal split is found by employing measures like the entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "data, labels = iris['data'], iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['target_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shannon_entropy(prob):\n",
    "    new_prob = prob[prob != 0.0]\n",
    "    return - np.sum(new_prob * np.log(new_prob))\n",
    "\n",
    "def split_data(data, feature_indx, split_param):\n",
    "    left_indcs = np.argwhere(data[:, feature_indx] <= split_param).T[0]\n",
    "    right_indcs = np.argwhere(data[:, feature_indx] > split_param).T[0]\n",
    "    return left_indcs, right_indcs\n",
    "\n",
    "def probabilities(left_indcs, right_indcs, labels):\n",
    "    left_prob = np.array([np.count_nonzero(labels[left_indcs] == c) for c in range(3)]) / len(left_indcs)\n",
    "    right_prob = np.array([np.count_nonzero(labels[right_indcs] == c) for c in range(3)]) / len(right_indcs)\n",
    "    return left_prob, right_prob\n",
    "\n",
    "def loss(data, labels, feature_indx, split_param):\n",
    "    left_indcs, right_indcs = split_data(data, feature_indx, split_param)\n",
    "    if len(left_indcs) == 0 or len(right_indcs) == 0:\n",
    "        return np.float64('Inf')\n",
    "    else:\n",
    "        left_prob, right_prob = probabilities(left_indcs, right_indcs, labels)\n",
    "        return (len(left_indcs) * shannon_entropy(left_prob) + len(right_indcs) *  shannon_entropy(right_prob)) / len(data)\n",
    "\n",
    "def optimal_split(data, labels):\n",
    "    losses = np.array([[loss(data, labels, ind, split) for split in data[:,ind]] for ind in range(4)])\n",
    "    optima = np.nonzero(losses == np.min(losses))\n",
    "    rnd_optimum_ind = np.random.choice(len(optima[0]))\n",
    "    opt_feature_indx = optima[0][rnd_optimum_ind]\n",
    "    opt_splt = data[optima[1][rnd_optimum_ind], opt_feature_indx]\n",
    "    return opt_feature_indx, opt_splt\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature_indx=None, splt=None, left=None, right=None, *, value=None):\n",
    "        self.feature_indx = feature_indx\n",
    "        self.splt = splt\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return self.value is not None or (self.left is None and self.right is None)\n",
    "\n",
    "def build_tree(data, labels):\n",
    "    if len(np.unique(labels)) == 1:\n",
    "        return Node(value=labels[0])\n",
    "    if len(labels) == 1:\n",
    "        return Node(value=labels[0])\n",
    "\n",
    "    opt_feature_indx, opt_splt = optimal_split(data, labels)\n",
    "\n",
    "    left_mask = data[:,opt_feature_indx] <= opt_splt\n",
    "    right_mask = ~left_mask\n",
    "\n",
    "    left_child = build_tree(data[left_mask,:], labels[left_mask])\n",
    "    right_child = build_tree(data[right_mask,:], labels[right_mask])\n",
    "\n",
    "    return Node(feature_indx=opt_feature_indx, splt=opt_splt, left=left_child, right=right_child)\n",
    "\n",
    "def predict_one(x, node):\n",
    "    # Base case: if leaf, return stored value\n",
    "    if node.is_leaf():\n",
    "        return node.value\n",
    "    \n",
    "    # Otherwise follow the split\n",
    "    if x[node.feature_indx] <= node.splt:\n",
    "        return predict_one(x, node.left)\n",
    "    else:\n",
    "        return predict_one(x, node.right)\n",
    "    \n",
    "def tree_pred(tree, data):\n",
    "    return np.array([predict_one(x, tree) for x in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = build_tree(data_train, labels_train)\n",
    "labels_pred_tree = tree_pred(tree, data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works! Apparently, decision trees can be highly sensitive under adding or removing a data point. That is where random forests come into play that allow to create an ensemble of decision trees to find a better balance of predictions. Also, by splitting the data into training and test sets, we can study whether the random forests reduce variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_indcs = np.random.choice(150, 150, replace=False)\n",
    "data_train = data[rand_indcs[:120],:]\n",
    "labels_train = labels[rand_indcs[:120]]\n",
    "data_test = data[rand_indcs[120:],:]\n",
    "labels_test = labels[rand_indcs[120:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we do the bagging $100$ times and each time, we randomly choose $90$ data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_random_forest(data, labels, n_bags, n_samples):\n",
    "    forest = []\n",
    "    for b in range(n_bags):\n",
    "        rand_indcs = np.random.choice(len(data), n_samples, replace=True)\n",
    "        bagged_data = data[rand_indcs,:]\n",
    "        bagged_labels = labels[rand_indcs]\n",
    "        forest.append(build_tree(bagged_data, bagged_labels))\n",
    "    return forest\n",
    "\n",
    "def forest_pred(forest, data):\n",
    "    pred_ensemble = np.zeros((len(forest), len(data)))\n",
    "    for (t,tree) in enumerate(forest):\n",
    "        pred_ensemble[t,:] = tree_pred(tree, data)\n",
    "    pred = np.round(np.mean(pred_ensemble, axis=0))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = build_random_forest(data_train, labels_train, 20, 110)\n",
    "labels_pred_forest = forest_pred(forest, data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the accuracy scores of both algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the random forest prediction on the Iris data set is 90.0 %\n",
      "Accuracy of the singe tree prediction on the Iris data set is 86.66666666666667 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the random forest prediction on the Iris data set is\", 100 * (1 - np.count_nonzero(labels_pred_forest - labels_test) / len(labels_test)), \"%\")\n",
    "print(\"Accuracy of the singe tree prediction on the Iris data set is\", 100 * (1 - np.count_nonzero(labels_pred_tree - labels_test) / len(labels_test)), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the data set is too small or too clean to show the effect of the random forest and the reduced variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
